\chapter{Controller Design and Evaluation}
In this chapter a control system is designed and evaluated to stabilize FLUTEs RF system.
Referring back to the block diagram of a generic control system in \autoref{fig:theoreticalFramework-feedback-architecture}, there are three blocks to determine.
First the plant transfer function, which describes the system that is to be controlled.
Second, based on the plant, an appropriate controller type is chosen and its parameters are calculated.
Third a measurement filter is used to improve the quality of the feedback signal path. As the choice of the measurement filter influences the controller design, its design is treated before the controller.

\section{Plant Identification}
\subsection{Principle}
Before choosing an appropriate controller, some insight of the system response has to be obtained. Therefore in this section the plants transfer function is estimated. In the context of this chapter ``plant'' refers to everything from the attenuation set at the controllable attenuator to the system output, e.g. the cavity RF power.

In the time domain, a LTI system is described by its impulse response $h(t)$, that is the reaction of the plant to an impulse at the input.
Using this definition directly, the plants impulse response $p(t)$ could be measured by applying a short peak in the attenuation setting on the attenuator. The effect on the output is not easy to measure and a single measurement of this kind is very susceptible to noise.
Therefore it is more common to measure the step response, which is the output of a system, when a step function is applied to its input. As the step function is the time integral of the impulse function, the step response can be converted to the impulse response by differentiation in time.

Instead of measuring a single step response, often several step responses are measured and their average is computed to reduce the variance of the estimation. When measuring a step response the minimum needed measuring time depends on the systems time constants, but they are often not known beforehand.

That is why when there is no prior knowledge of the system, the identification is sometimes done with a (pseudo) random binary sequence to excite the system with step functions of different lengths. The PRBS is chosen in a way that that some of the steps will probably last longer than a few dominant time constants of the system. 

To get the transfer function $P(s)=\mathcal{L}\left\{p(t)\right\}$ of the plant from its step response(s), several methods are common, including correlation based and frequency response based algorithms.

\subsection{Identifying the Plant Attenuator+RF}
The input PRBS signal is generated with the Python script in \autoref{fig:controllerDesignAndEvaluation-randomSequence}.
Based on the value of a pseudo random number generator, the sequence toggles the attenuator between $V_\text{control}=\SI{7}{\volt}$ and $V_\text{control}=\SI{7}{\volt}$.
Using \autoref{fig:interfacingFlute_attenDatten}, this equals a span in attenuation of \SIrange{6.892}{6.4026}{\dB}.
With the parameter \texttt{toggleP}, the average length of one constant voltage level can be controlled. 

\begin{lstlisting}[style=python,caption = Function to get a pseudo random binary sequence, label = lst:controllerDesignAndEvaluation-randomSequence]
def randomBinarySequence(N,toggleP):
    u=[False]*N
    for i in range(1,len(u)):
        if(np.random.binomial(1,toggleP,1)[0]):
            u[i]=not u[i-1]
        else:
            u[i]=u[i-1]
    return list(map(lambda x: 7 if x==False else 11,u))
\end{lstlisting}

In a test run over six hours (after all FLUTE subsystems had stabilized), the attenuator was driven with such a PRBS. The result is shown in \autoref{fig:controllerDesignAndEvaluation-identify-excitation}.

\begin{figure}[tb]
	\centering
	\includegraphics[width=\textwidth,height=0.5\textwidth]{chap/ControllerDesignAndEvaluation/img/identification/excitation.tikz}
	\caption{Section of the input sequence (blue) and the system response (orange); Note the inverse relation: A higher attenuation $A$ causes a lower cavity power $P_\text{cavity}$}
	\label{fig:controllerDesignAndEvaluation-identify-excitation}
\end{figure}

The time signals $A(t)$ and $P_\text{cavity}(t)$ are then split into a \textit{estimation} data set (about \SI{80}{\percent} of the samples) and a \textit{validation} data set (the remaining $\approx$ \SI{20}{\percent}). This is done so the bulk of the available information is used to estimate the model, but there is data left that the model hasn't seen before. This smaller portion is used to validate the models performance, hence it is called the validation data set.

The two data sets are then loaded into the \textsc{Matlab} \textit{System Identification Toolbox}. With the toolboxes pre-processing tools, first the means of both the input and output are removed, which is required by the estimators used.
Then, using the ``process model'' estimator, continuous time transfer functions with different numbers of zeros and poles are estimated. After that, to check the accuracy of the estimations, the System Identification Toolbox is used to simulate the output of the estimated systems (see \autoref{fig:controllerDesignAndEvaluation-identify-mo}). For that the aforementioned validation data set is used: The measured output is compared with the outputs predicted by the models.

Using the Matlab function \texttt{zpk()} the models are converted into the zero-pole-gain representation. In
\autoref{tab:controllerDesignAndEvaluation-identify-models} the estimated models are listed with their zeros, poles, gains and the model fit, as it is computed by the System Identification Toolbox. \texttt{P1}, \texttt{P2} and \texttt{P3} are models with one, two and three poles and a gain as free parameters. \texttt{P2ZU} consists of a complex pole pair, a zero and a gain. 

\begin{figure}[tb]
	\centering
	\includegraphics[width=\textwidth,height=0.5\textwidth]{chap/ControllerDesignAndEvaluation/img/identification/modelOutputs.tikz}
	\caption{Validation of the estimated process models for the plant; the legend also shows the model fits in percent}
	\label{fig:controllerDesignAndEvaluation-identify-mo}
\end{figure}

\begin{table}[tb]
\caption{Process models of the plant as estimated by the Matlab System Identification Toolboxes process model estimator}
\label{tab:controllerDesignAndEvaluation-identify-models}
\centering
\begin{tabular}{lcccc}
\toprule
Model name & Zeros & Poles & Gain & Model fit \\
\midrule
P1   & & $(s+0.3505)$ & $-646.94$ & \SI{89.98}{\percent}\\
P2   & & $(s+0.6875) (s+0.7039)$ & $-873.18$ & \SI{93.83}{\percent}\\
P3   & & $(s+\num{1e06}) (s+0.7376) (s+0.669)$ & $\num{-8.9019e+08}$ & \SI{93.85}{\percent}\\
P2ZU &$(s-2846)$ & $(s^2 + 0.8014s + 0.3195)$ & $0.20296$ & \SI{96.26}{\percent}\\
\bottomrule
\end{tabular}
\end{table}

\autoref{fig:controllerDesignAndEvaluation-identify-mo} and \autoref{tab:controllerDesignAndEvaluation-identify-models} show the \texttt{P2ZU} model to have the best fit. Therefore it is accepted as the plants transfer function. Using the Matlab function \texttt{tf()}, its time continuous transfer function can be stated as
\begin{equation}
P(s) = \frac{0.6352s - 1808}{3.13 s^2 + 2.508 s + 1}.
\end{equation}
Using the Matlab function \texttt{c2d()}\footnote{Without specifying a different method, \texttt{c2d()}  discretizes the continuous-time model zero-order hold on the input.} and the sample time $T_s=\SI{0.2}{\second}$, $G(s)$ can be converted to the time discrete transfer function
\begin{equation}
P(z) = \frac{-10.91 z^{-1} - 10.41 z^{-2}}{1 - 1.84 z^{-1} + 0.8519 z^{-2}}.
\end{equation}










\newpage
\section{Measurement Filter}
Like all measurements of physical quantities, the measuring of the system output of the control system is subjected to noise.
In addition to these disturbances of thermal or electrical origins, also high frequency variations of the system output has detrimental effects on the control systems performance.
For example the magnitudes of the bunch-by-bunch changes of the measured cavity power are often in the same order as the long-term drifts.
Trying to correct for them instead of the long term drifts often leads to overcompensating and can even make the system unstable.

To remove the high frequency components a low pass filter is used as the measurement block $H(s)$.

In pre-tests the incoming signal was simply filtered with a moving average filter.
Commonly, the moving average is defined as the mean of a signal $x$ inside a window of length $L$, centered around the current time or sample index $n$, that is shifted along the signal. This smooths out small variations thus the moving average acts as a low pass filter.
This \textit{non-causal} version of the moving average can only be used with already measured data as to compute the moving average at $n$, future values at $n+i$ are needed:
\begin{equation}
\operatorname{MA}_{x,\text{non-causal},L}[n] = \frac{1}{L} \sum_{i=n-\frac{L-1}{2}}^{n+\frac{L-1}{2}} x[i]
\end{equation}
When filtering real-time data, future values are not available and a shifted, \textit{causal} version, of the moving average
\begin{equation}\label{eq:causalMA}
\operatorname{MA}_{x,\text{causal},L}[n] = \frac{1}{L} \sum_{i=n-(L-1)}^{n} x[i]
\end{equation}
is used.

In case of the cavity RF power, experiments show a window length of about $L=100$ or more is necessary to sufficiently smooth the measured power signal.
When comparing the original signal with the filtered one, it is apparent that in addition to the desired smoothing effect, the filtered signal also is delayed in time, with the delay being dependent on the window size.
To quantify the delay, the alternative definition of the moving average as a digital FIR filter is used.
One possibility to describe a FIR filter is by giving its impulse response, i.e. the output signal when the input of the filter is an impulse with unity height. In case of the moving average filter, the coefficient sequence of the corresponding FIR filter has the length $N=L$ and is equal to the the impulse response $h[n]$:
\begin{equation}
h[n] = \frac{1}{N} [\underbrace{1,\,1,\,...,\,1}_N]
\end{equation}

The delay introduced by a digital filter can be quantified with the filters group delay
\begin{equation}
\frac{\tau_g(f)}{T_s} = \frac{\d{\phi(f)}}{\d{f}}
\end{equation}
which is given normalized to the sampling time $T_s$ \cite[p.~70]{Kammeyer2002}. In case of a FIR filter with linear phase (with a symmetrical impulse response), the group delay is always constant over all frequencies and is only dependent on the filter length $N$\cite[p.~165]{Kammeyer2002}:
\begin{equation}\label{eq:groupD}
\frac{\tau_g(f)}{T_s} = \frac{\d{\phi(f)}}{\d{f}} = \frac{\d{\phi}}{\d{f}} = \frac{N}{2}
\end{equation}

With a sampling time of $T_s=\nicefrac{1}{\SI{5}{\hertz}}=\SI{200}{\milli\second}$ and $N=100$ the group delay is \SI{10}{\second}. In case of a steady operation this is acceptable, as the disturbances to compensate happen on a timescale in the order of several minutes.
But in case of ongoing transients in case of user changes to the control system parameters, or short error bursts on the measured signal, this long delay causes problems and therefore should be reduced.

Therefore a more sophisticated digital filter is designed to replace the simple moving average.

On the one hand, a FIR filter is designed with the Kaiser window method.
This method starts with the desired frequency response, which is usually given piece-wise. 
In case of the low pass filter it is a step function at a cutoff frequency $f_c$.
Then the IDFT is used to compute the corresponding impulse response $h_\text{IIR}[n]$, which is in general infinitely long.
Windowing with e.g. a Kaiser window and then truncating the impulse response then yields the impulse response of the desired FIR filter $h_\text{FIR}[n]$.\cite[p.~533]{Oppenheim2010}.
With SciPy using \texttt{b=signal.firwin(N, fc,fs)}, the coefficients of a FIR with this method can be calculated.

On the other hand, an IIR filter is designed with the impulse invariance method and an analogue Butterworth filter.
This method could be interpreted as sampling the infinitely long analogue impulse response.\cite[p.~497]{Oppenheim2010}
With SciPy using \texttt{b,a=signal.butter(N,fc,'lowpass',fs,)}, the coefficients of an IIR with this method can be calculated.
For an IIR filter, the group delay cannot be calculated with \autoref{eq:groupD} and it is in general frequency depend.

\autoref{fig:controllerDesignAndEvaluation-impulseresponses} shows the impulse responses of the moving average, the FIR lowpass and the IIR lowpass (truncated to $N=100$).

\begin{figure}[tb]
	\centering
	\includegraphics[width=\textwidth,height=0.5\textwidth]{chap/ControllerDesignAndEvaluation/img/measurementFilter/firVsMa_impulseresponses.tikz}
	\caption{Impulse responses of a moving average filter ($N=100$), a FIR lowpass ($N=50$, $f_c=\SI{0.1}{\hertz}$) and a IIR Butterworth lowpass ($N=50$, $f_c=\SI{0.1}{\hertz}$)}
	\label{fig:controllerDesignAndEvaluation-impulseresponses}
\end{figure}

In \autoref{fig:controllerDesignAndEvaluation-filterResults}, the three filter types described above are compared by filtering a ten minute long segment of pre-recorded data. The filtering is done with the SciPy function \texttt{signal.lfilter()} which does causal filtering and does not compensate group delay\footnote{In contrast to \texttt{signal.filtfilt()}, which applies the filter both forward and backward achieving zero phase/group delay, but this cannot be done for incoming real-time data.}, so the results are the same as they would be for real-time data.

\begin{figure}[tb]
	\centering
	\includegraphics[width=\textwidth,height=0.5\textwidth]{chap/ControllerDesignAndEvaluation/img/measurementFilter/firVsMa.tikz}
	\caption{Effects of the three different lowpass filters in \autoref{fig:controllerDesignAndEvaluation-impulseresponses} on noisy data}
	\label{fig:controllerDesignAndEvaluation-filterResults}
\end{figure}

The plot shows the FIR lowpass filter requiring ten times the number of coefficients to achieve about the same result as the IIR lowpass filter.
Also the moving average filter has double the number of coefficients as the FIR lowpass filter, but there is still high frequency noise in the output (caused by the $\text{sinc}(\cdot)$ shape of its frequency response $H[f]=\operatorname{DFT}\{h[n]\}$).

Compared to the FIR lowpass, the moving average offers no benefit besides its easy implementation.
When comparing the FIR with the IIR approach, the IIR has the advantage of needing less coefficients, thus occupying less memory, which is not really an advantage when the control system is implemented on a personal computer, which typically has enough free memory to hold millions of floating point numbers.
Also the IIR filter has a non-constant group delay and is not guaranteed to be stable like all FIR filters are.

For these reasons, in the following an FIR lowpass filter is used.

\paragraph{Real-time implementation of a FIR Filter in Python}
Similar to \autoref{eq:causalMA}, a FIR filter designed with the SciPy function \texttt{sigal.firwin()} can be used in a causal manner to filter real-time data.

Applying the filter on pre-recorded data, like in \autoref{fig:controllerDesignAndEvaluation-filterResults} can be done with \texttt{signal.lfilter()}. To use \texttt{signal.lfilter()} on sample-wise incoming real-time data, the ``initial in'' input and ``final out'' output of \texttt{signal.lfilter()} can be used to keep the filters state. This is demonstrated in \autoref{lst:controllerDesignAndEvaluation-zizf} by looping through pre-recorded data point-by-point.

\begin{lstlisting}[style=python,caption = Demonstration of the \texttt{zi} and \texttt{zf} variables when using \texttt{signal.lfilter()}, label = lst:controllerDesignAndEvaluation-zizf]
x=df2["F:RF:LLRF:01:GunCav1:Power:Out Value"].to_numpy()
y=np.array([])
zf=signal.lfilter_zi(b, 1)
for i in range(len(x)):
    y0,zf=signal.lfilter(b, 1, [x[i]],zi=zf)
    y=np.append(y,y0[0])
\end{lstlisting}









\newpage
\section{Controller Design}
For many control problems, especially if the plant behaves approximately as an LTI system and the system is of low order, a simple PID (proportional, integral and derivative) controller is a good starting point (visualized in \autoref{fig:own-work-pid-block}). 

A PID controller uses the error $e(t)$, the temporal integral of the error $e_i(t)$ and the temporal derivative of the error $e_d(t)$ as an input and outputs a weighed sum of them. While the unmodified error signal represents the current error, the integrated and derived error signals allow to controller to ``see'' in the past and predict the future.

Often simplifications, such as a pure P (only $k_p \neq 0$) or a PI (only $k_p,\,k_i \neq 0$) controller are valid as well. Since the plant has been identified to be a second order system, a simple P controller is not enough to bring the steady state error to zero (see ). So at least a PI controller is needed.

\begin{figure}[tb]
	\centering
	\includegraphics[width=0.8\textwidth]{chap/ControllerDesignAndEvaluation/img/controller/pid.tikz}
	\caption{Block diagram of a generic PID controller}
	\label{fig:own-work-pid-block}
\end{figure}

As a starting point for software development and parameter tuning, the parameters $k_p=0.00001$ and $k_i=k_d=0$ are chosen. This ensures during development the system basically does nothing but still shows changing values at the controller output.


\section{Software Design}
As integrating a new subsystem into EPICS takes some time and effort and the control system is designated a temporary solution, it is more viable to operate it as an independent system.

Before choosing a programming language, software frameworks, etc. the key requirements for the software are discussed:
\begin{itemize}
\item Communication with EPICS to get values and with the RF attenuator
\item Efficient and lightweight to achieve clock cycles times of a most \SI{0.1}{\second}
\item Easy implementation of a (time discrete) PID controller
\item GUI to show input, output and error signals
\item Possibility to log signals to file for documentation
\end{itemize}

With these in mind first programming languages are regarded. As there are EPICS libraries for both C++ and Python, these two languages are examined in more detail.\\
While C++ as a compile language promises speed, all other requirements are possible but would take much greater effort in C++ compared to Python. For that reason, in the following a small test program is written to evaluate the fastest clock cycle possible with a simple Python program.

 shows that retrieving one value of an EPICS channel and setting a new attenuator voltage takes only about \SI{20}{\milli\second}, thus using C++ is not necessary and Python can be utilized instead.
 
To create a PID controller in software instead of a continuous time system, only discrete time implementations are possible. Choosing a high clock cycle frequency however approximates the continuous time system. To get the error signals for the integral and derivative part, the integral is replaced with a cumulative recursive sum as
\begin{equation}
e_i[n]=e_i[n-1]+e[n] \cdot dt,
\end{equation}
while the derivative is replaced by a difference
\begin{equation}
e_d[n]=\frac{e_i[n-1]-e[n]}{dt}.
\end{equation}

For the GUI a common framework should be used. In Python Tk and wxwidgets are common ways to build a GUI. Another viable option is PyQt, which, as the name suggests, is a Python port of the Qt framework. One major advantage of using PyQt is the possibility to import \texttt{.ui} files describing the GUI directly from the Qt RAD designer called ``Qt designer'', removing the need to create the GUI programmatically. Furthermore using PyQt enables the usage of \textit{pyqtgraph}, a highspeed plotting library only compatible with Qt. This ensures plotting live data does not bottleneck performance, which is often the problem with naive \textit{matplotlib} based solutions.

To log all relevant data from RAM to non-volatile memory (hard drive or network share), a simple approach with a line by line CSV writer is used.







\section{Control Parameter Tuning and Tests}
To tune control parameters there are a multiple of analytical, empirical or hybrid approaches. Here the Ziegler-Nichols method is tested and fine tuning is done by hand.

\section{Improving the Control System}

\subsection{Adding Disturbance Feed Forward}















\newpage
\section{Results}
To evaluate the performance of the control system, FLUTE is operated with and without the control system switched on for \SI{6}{\hour} each.
Before the test, FLUTE is allowed to run a few hours for all components to reach operating temperatures.
The result of the test is shown in \autoref{fig:controllerDesignAndEvaluation_resultTime}.
Note, that in this test about \SI{7}{\hour} into the experiment there was an unintentional shutdown of FLUTE and the corresponding block of data is removed before further processing.

\begin{figure}[tbh]
	\centering
	\includegraphics[width=\textwidth,height=0.5\textwidth]{chap/ControllerDesignAndEvaluation/img/results/result_timesignal.tikz}
	\caption{Cavity power over about \SI{15}{\hour} (about three hours of downtime removed for clarity around the \SI{7}{\hour} mark); control system switched off at \SI{6}{\hour} (recording started 2021/05/01 20:00)}
	\label{fig:controllerDesignAndEvaluation_resultTime}
\end{figure}

\begin{figure}[tbh]
	\centering
	\includegraphics[width=\textwidth,height=0.5\textwidth]{chap/ControllerDesignAndEvaluation/img/results/result_spectrogram.tikz}
	\caption{Spectrogram of the cavity power in \autoref{fig:controllerDesignAndEvaluation_resultTime}}
	\label{fig:controllerDesignAndEvaluation_resultSpectg}
\end{figure}

The time plot and also the spectogram in \autoref{fig:controllerDesignAndEvaluation_resultSpectg} shows the cavity RF power approximately reaches stationarity in $[0,\,2] \si{hour}$ respectivaley $[6,\,12] \si{hour}$. This allows the spectrum for these two blocks to be estimated using a periodogram method. In \autoref{fig:controllerDesignAndEvaluation_resultSpectg} the spectra for each case are shown.

\begin{figure}[tbh]
	\centering
	\includegraphics[width=\textwidth,height=0.7\textwidth]{chap/ControllerDesignAndEvaluation/img/results/controllerOnVsOff.tikz}
	\caption{Comparison between the control system on and off}
	\label{fig:controllerDesignAndEvaluation_resultComp}
\end{figure}

\begin{figure}[tbh]
    \centering
        \subfloat[Controller off]{\includegraphics[width=\textwidth,height=0.4\textwidth]{chap/ControllerDesignAndEvaluation/img/stdp_off.tikz}}
        \\
        \subfloat[Controller on]{\includegraphics[width=\textwidth,height=0.4\textwidth]{chap/ControllerDesignAndEvaluation/img/stdp_on.tikz}}
       \caption{Relative standard deviation $STD\%(t,T)$}
    \label{fig:controllerDesignAndEvaluation_resultSTDp}
\end{figure}

\begin{figure}[tbh]
	\centering
	\includegraphics[width=\textwidth,height=0.5\textwidth]{chap/ControllerDesignAndEvaluation/img/stdp_means.tikz}
	\caption{Relative standard deviation $STD\%(T)$, shaded areas show $\operatorname{min}\{STD\%(t,T_0)\}$ and $\operatorname{max}\{STD\%(t,T_0)\}$, solid lines show $\operatorname{mean}\{STD\%(t,T_0)\}$ for a fixed window size $T=T_0$}
	\label{fig:controllerDesignAndEvaluation_resultSTDp_means}
\end{figure}


\begin{figure}[tbh]
	\centering
	\includegraphics[width=\textwidth,height=0.7\textwidth]{chap/ControllerDesignAndEvaluation/img/results/controllerOnVsOffSpectrumErrorBand.tikz}
	\caption{Power spectrum of the plots in \autoref{fig:controllerDesignAndEvaluation_resultTime} computed with Welch's method; shaded areas show the uncertainty according to \autoref{eq:varWelch}}
	\label{fig:controllerDesignAndEvaluation_resultPeriodo}
\end{figure}

With the metrics from \autoref{sec:metrics}, the success of the control system is assessed in \autoref{tab:controllerDesignAndEvaluation_metrics}.

This shows for example the mean squared error is improved by a factor of about \num{371} by using the control system.

\begin{table}[tbh]
\centering
\caption{Quantitative assessment of the controllers performance}\label{tab:controllerDesignAndEvaluation_metrics}
\begin{tabular}{lSSS}
	\toprule
	Metric  & {Controller off} & {Controller on} & {Controller off/Controller on} \\ \midrule
	$\%STD$ & 0.0011559        & 5.99225e-05     & 9.2891                         \\
	$MSE$   & 37.639           & 0.10133         & 371.44                         \\
	$MPN$   & 487309.29        & 14386.25        & 33.873                         \\ \bottomrule
\end{tabular}
\end{table}














