\chapter{Controller Design and Evaluation}


\section{Plant Identification}
\subsection{Principle}
Before choosing an appropriate controller, some insight of the system response has to be obtained. For that reason, next the plant transfer function is obtained. In the time domain, the transfer function is the response of the system to an impulse on the input. So per definition, in the special case here, this would mean changing the RF attenuator quickly from a big attenuation to a small attenuation and then back. This is not easy to measure and a single measurement is very susceptible to noise. Therefore it is more common to measure the step response instead and to average over several step responses.

When there is no prior knowledge over the system, the identification is sometimes done with a (pseudo) random binary sequence to excite the system with step functions of different lengths. Then it is necessary that some of the steps last longer than a few dominant time constants of the system. To get the transfer function of the system from its step response, several methods are common, including correlation based and frequency response based algorithms.

\subsection{Identifying the system response of the FLUTE LLRF}
The input sequence is generated by modulating the RF attenuator around a base attenuation. As a trade off between high SNR and driving the LLRF in a ``save'' region, the control voltage span is chosen to be \SI{4}{\volt} in total (\SIrange{7}{11}{\volt} around the base control voltage of \SI{9}{\volt}).

To get a random binary sequence, depending on the outcome of a binomial random process, the voltage is toggled between \SI{7}{\volt} and \SI{11}{\volt} according to \autoref{lst:own-work-randomSequence}. With the parameter \texttt{toggleP}, the average length of one constant voltage level can be controlled.

\begin{lstlisting}[style=python,caption = Function to get a random binary sequence, label = lst:own-work-randomSequence]
def randomBinarySequence(N,toggleP):
    u=[False]*N
    for i in range(1,len(u)):
        if(np.random.binomial(1,toggleP,1)[0]):
            u[i]=not u[i-1]
        else:
            u[i]=u[i-1]
    return list(map(lambda x: 7 if x==False else 11,u))
\end{lstlisting}

In a test run over 6 hours (after FLUTE had stabilized), the attenuator was driven with such a random sequence. The result is shown in \autoref{fig:own-work-identify-input}.

\begin{figure}[tb]
	\centering
	\includegraphics[width=\textwidth,height=0.5\textwidth]{chap/ControllerDesignAndEvaluation/img/controller/time.tikz}
	\caption{Small section of the input sequence (green) and the system response (blue)}
	\label{fig:own-work-identify-input}
\end{figure}

The time signals are then split into a estimation data set (about \SI{80}{\percent} of samples) and a validation data set (the remaining $\approx$ \SI{20}{\percent}).

The two data sets are then loaded into the \textsc{Matlab} \textit{System Identification Toolbox} (SIT). With the SIT, first the means of both sets and both the input and output are removed, which is required by the estimators used. Then with different numbers of poles and zeros, the transfer function is estimated. After trying several settings, three promising candidates

\begin{align}
\hat{G}_1(s) &= \frac{71.37\,s + 0.5966}{s^3 + 0.8208\,s^2 + 0.328\,s + 0.002733} \\[1em]
\hat{G}_2(s) &= \frac{-0.2125\,s + 70.85}{s^2 + 0.8022\,s + 0.3202} \\[1em]
\hat{G}_3(s) &= \frac{79.12}{s + 0.3502}
\end{align}
emerge.

For these transfer functions, the (single) step responses are given in \autoref{fig:own-work-identify-step}.

\begin{figure}[tb]
	\centering
	\includegraphics[width=\textwidth,height=0.5\textwidth]{chap/ControllerDesignAndEvaluation/img/controller/steps.tikz}
	\caption{Step responses of the systems $\hat{G}_1(s)$, $\hat{G}_2(s)$, $\hat{G}_3(s)$}
	\label{fig:own-work-identify-step}
\end{figure}

To check the accuracy of the estimations, the SIT is used to do a simulation with the $\hat{G}_i(s)$ and the validation data set (see \autoref{fig:own-work-identify-mo}).

\begin{figure}[tb]
	\centering
	\includegraphics[width=\textwidth,height=0.5\textwidth]{chap/ControllerDesignAndEvaluation/img/controller/mo2.tikz}
	\caption{Validating the estimations $\hat{G}_1(s)$, $\hat{G}_2(s)$, $\hat{G}_3(s)$ against real data from the validation data set}
	\label{fig:own-work-identify-mo}
\end{figure}

\autoref{fig:own-work-identify-mo} shows that a first order system with one pole and no zeros is not a sufficient approximation as there is no overshoot, since it is not possible in a first order system. The second and third order systems $\hat{G}_2(s)$ and $\hat{G}_3(s)$ show much better fits.

The important conclusion of this section is that the plant can be modeled as a \textbf{second order system}. This info is needed when choosing a controller in the next section.



\section{Measurement Filter}
Like all measurements of physical quantities, the measuring of the system output of the control system is subjected to noise.
In addition to these disturbances of thermal or electrical origins, also high frequency variations of the system output has detrimental effects on the control systems performance.
For example the magnitudes of the bunch-by-bunch changes of the measured cavity power are often in the same order as the long-term drifts.
Trying to correct for them instead of the long term drifts often leads to overcompensating and can even make the system unstable.

To remove the high frequency components a low pass filter is used as the measurement block $XX$.

In pre-tests the incoming signal was simply filtered with a moving average filter.
Commonly, the moving average is defined as the mean of a signal $x$ inside a window of length $L$, centered around the current time or sample index $n$, that is shifted along the signal. This smooths out small variations thus the moving average acts as a low pass filter.
This \textit{non-causal} version of the moving average can only be used with already measured data as to compute the moving average at $n$, future values at $n+i$ are needed:
\begin{equation}
\operatorname{MA}_{x,\text{non-causal},L}[n] = \frac{1}{L} \sum_{i=n-\frac{L-1}{2}}^{n+\frac{L-1}{2}} x[i]
\end{equation}
When filtering real-time data, future values are not available and a shifted, \textit{causal} version, of the moving average
\begin{equation}\label{eq:causalMA}
\operatorname{MA}_{x,\text{causal},L}[n] = \frac{1}{L} \sum_{i=n-(L-1)}^{n} x[i]
\end{equation}
is used.

In case of the cavity RF power, experiments show a window length of about $L=100$ or more is necessary to sufficiently smooth the measured power signal.
When comparing the original signal with the filtered one, it is apparent that in addition to the desired smoothing effect, the filtered signal also is delayed in time, with the delay being dependent on the window size.
To quantify the delay, the alternative definition of the moving average as a digital FIR filter is used.
One possibility to describe a FIR filter is by giving its impulse response, i.e. the output signal when the input of the filter is an impulse with unity height. In case of the moving average filter, the coefficient sequence of the corresponding FIR filter has the length $N=L$ and is equal to the the impulse response $h[n]$:
\begin{equation}
h[n] = \frac{1}{N} [\underbrace{1,\,1,\,...,\,1}_N]
\end{equation}

The delay introduced by a digital filter can be quantified with the filters group delay
\begin{equation}
\frac{\tau_g(f)}{T_s} = \frac{\d{\phi(f)}}{\d{f}}
\end{equation}
which is given normalized to the sampling time $T_s$ \cite[p.~70]{Kammeyer2002}. In case of a FIR filter with linear phase (with a symmetrical impulse response), the group delay is always constant over all frequencies and is only dependent on the filter length $N$\cite[p.~165]{Kammeyer2002}:
\begin{equation}\label{eq:groupD}
\frac{\tau_g(f)}{T_s} = \frac{\d{\phi(f)}}{\d{f}} = \frac{\d{\phi}}{\d{f}} = \frac{N}{2}
\end{equation}

With a sampling time of $T_s=\nicefrac{1}{\SI{5}{\hertz}}=\SI{200}{\milli\second}$ and $N=100$ the group delay is \SI{10}{\second}. In case of a steady operation this is acceptable, as the disturbances to compensate happen on a timescale in the order of several minutes.
But in case of ongoing transients in case of user changes to the control system parameters, or short error bursts on the measured signal, this long delay causes problems and therefore should be reduced.

Therefore a more sophisticated digital filter is designed to replace the simple moving average.

On the one hand, a FIR filter is designed with the Kaiser window method.
This method starts with the desired frequency response, which is usually given piece-wise. 
In case of the low pass filter it is a step function at a cutoff frequency $f_c$.
Then the IDFT is used to compute the corresponding impulse response $h_\text{IIR}[n]$, which is in general infinitely long.
Windowing with e.g. a Kaiser window and then truncating the impulse response then yields the impulse response of the desired FIR filter $h_\text{FIR}[n]$.\cite[p.~533]{Oppenheim2010}.
With SciPy using \texttt{b=signal.firwin(N, fc,fs)}, the coefficients of a FIR with this method can be calculated.

On the other hand, an IIR filter is designed with the impulse invariance method and an analogue Butterworth filter.
This method could be interpreted as sampling the infinitely long analogue impulse response.\cite[p.~497]{Oppenheim2010}
With SciPy using \texttt{b,a=signal.butter(N,fc,'lowpass',fs,)}, the coefficients of an IIR with this method can be calculated.
For an IIR filter, the group delay cannot be calculated with \autoref{eq:groupD} and it is in general frequency depend.

\autoref{fig:controllerDesignAndEvaluation-impulseresponses} shows the impulse responses of the moving average, the FIR lowpass and the IIR lowpass (truncated to $N=100$).

\begin{figure}[tb]
	\centering
	\includegraphics[width=\textwidth,height=0.5\textwidth]{chap/ControllerDesignAndEvaluation/img/measurementFilter/firVsMa_impulseresponses.tikz}
	\caption{Impulse responses of a moving average filter ($N=100$), a FIR lowpass ($N=50$, $f_c=\SI{0.1}{\hertz}$) and a IIR Butterworth lowpass ($N=50$, $f_c=\SI{0.1}{\hertz}$)}
	\label{fig:controllerDesignAndEvaluation-impulseresponses}
\end{figure}

In \autoref{fig:controllerDesignAndEvaluation-filterResults}, the three filter types described above are compared by filtering a ten minute long segment of pre-recorded data. The filtering is done with the SciPy function \texttt{signal.lfilter()} which does causal filtering and does not compensate group delay\footnote{In contrast to \texttt{signal.filtfilt()}, which applies the filter both forward and backward achieving zero phase/group delay, but this cannot be done for incoming real-time data.}, so the results are the same as they would be for real-time data.

\begin{figure}[tb]
	\centering
	\includegraphics[width=\textwidth,height=0.5\textwidth]{chap/ControllerDesignAndEvaluation/img/measurementFilter/firVsMa.tikz}
	\caption{Effects of the three different lowpass filters in \autoref{fig:controllerDesignAndEvaluation-impulseresponses} on noisy data}
	\label{fig:controllerDesignAndEvaluation-filterResults}
\end{figure}

The plot shows the FIR lowpass filter requiring ten times the number of coefficients to achieve about the same result as the IIR lowpass filter.
Also the moving average filter has double the number of coefficients as the FIR lowpass filter, but there is still high frequency noise in the output (caused by the $\text{sinc}(\cdot)$ shape of its frequency response $H[f]=\operatorname{DFT}\{h[n]\}$).

Compared to the FIR lowpass, the moving average offers no benefit besides its easy implementation.
When comparing the FIR with the IIR approach, the IIR has the advantage of needing less coefficients, thus occupying less memory, which is not really an advantage when the control system is implemented on a personal computer, which typically has enough free memory to hold millions of floating point numbers.
Also the IIR filter has a non-constant group delay and is not guaranteed to be stable like all FIR filters are.

For these reasons, in the following an FIR lowpass filter is used.

\paragraph{Real-time implementation of an FIR Filter in Python}
Similar to \autoref{eq:causalMA}, a FIR filter designed with the SciPy function \texttt{sigal.firwin()} can be used in a causal manner to filter real-time data.

Applying the filter on pre-recorded data, like in \autoref{fig:controllerDesignAndEvaluation-filterResults} can be done with \texttt{signal.lfilter()}. To use \texttt{signal.lfilter()} on sample-wise incoming real-time data, the ``initial in'' input and ``final out'' output of \texttt{signal.lfilter()} can be used to keep the filters state. This is demonstrated in \autoref{lst:controllerDesignAndEvaluation-zizf} by looping through pre-recorded data point-by-point.

\begin{lstlisting}[style=python,caption = Demonstration of the \texttt{zi} and \texttt{zf} variables when using \texttt{signal.lfilter()}, label = lst:controllerDesignAndEvaluation-zizf]
x=df2["F:RF:LLRF:01:GunCav1:Power:Out Value"].to_numpy()
y=np.array([])
zf=signal.lfilter_zi(b, 1)
for i in range(len(x)):
    y0,zf=signal.lfilter(b, 1, [x[i]],zi=zf)
    y=np.append(y,y0[0])
\end{lstlisting}



\section{Controller design}
For many control problems, especially if the plant behaves approximately as an LTI system and the system is of low order, a simple PID (proportional, integral and derivative) controller is a good starting point (visualized in \autoref{fig:own-work-pid-block}). 

A PID controller uses the error $e(t)$, the temporal integral of the error $e_i(t)$ and the temporal derivative of the error $e_d(t)$ as an input and outputs a weighed sum of them. While the unmodified error signal represents the current error, the integrated and derived error signals allow to controller to ``see'' in the past and predict the future.

Often simplifications, such as a pure P (only $k_p \neq 0$) or a PI (only $k_p,\,k_i \neq 0$) controller are valid as well. Since the plant has been identified to be a second order system, a simple P controller is not enough to bring the steady state error to zero (see ). So at least a PI controller is needed.

\begin{figure}[tb]
	\centering
	\includegraphics[width=0.8\textwidth]{chap/ControllerDesignAndEvaluation/img/controller/pid.tikz}
	\caption{Block diagram of a generic PID controller}
	\label{fig:own-work-pid-block}
\end{figure}

As a starting point for software development and parameter tuning, the parameters $k_p=0.00001$ and $k_i=k_d=0$ are chosen. This ensures during development the system basically does nothing but still shows changing values at the controller output.

\section{Inputs and Outputs}
Since the control algorithm should be implemented, tested and used online on the actual accelerator instead of only operate on simulated data, there is the need for fast and reliable interfaces to the machine.
Following, ``input'' refers to the signal going into the control algorithm (i.e. the measured $y(t)$), while ``output'' is the output of the control algorithm $u(t)$.

\subsection{Input}
Depending on which value is chosen to be controlled, filtering of the input signal could be mandatory.

In the case of the cavity RF power the signal jumps to zero each time a breakdown occurs, shortening the RF supply. These outliners are not representative of the average RF power inside the cavity over multiple pulses and thus would greatly impair the controller performance. For that reason, before any further filtering to remove noise etc, a breakdown removal filter is used (\autoref{lst:control-system-breakdownremoval}). In principle the new power value is checked to be inside a band which size is determined by the mean deviation of the $N_{filt}$ previous values and a scaling $m$. The percentile differences are used here as they are robust against outliners (i.e. other breakdowns) in the $N_{filt}$ previous values opposed to a normal standard deviation. The scaling with $(2*1.2815)^{-1}$ is used to make the mean deviation comparable to a standard deviation.

\begin{lstlisting}[style=python,caption = Breakdown removal, label = lst:control-system-breakdownremoval]
#...
if(abs(P[i]-np.median(P[i-3*Nfilt:i-Nfilt]))<
m*(np.percentile(P[i-3*Nfilt:i-Nfilt],90)-np.percentile(P[i-3*Nfilt:i-Nfilt],10)/(2*1.2815))):
   P_filt=np.append(P_filt,P[i])
else:
   breakdown_locations_predicted=np.append(breakdown_locations_predicted,i)
   P_filt=np.append(P_filt,np.median(P[i-3*Nfilt:i-Nfilt]))
#...
\end{lstlisting}




\subsection{Output}
For the control system to work the controller needs some way of influencing the plant. For that the output of the FLUTE LLRF vector modulator is controlled by a RF attenuator (see \autoref{chap:atten}).

\section{Software Design}
As integrating a new subsystem into EPICS takes some time and effort and the control system is designated a temporary solution, it is more viable to operate it as an independent system.

Before choosing a programming language, software frameworks, etc. the key requirements for the software are discussed:
\begin{itemize}
\item Communication with EPICS to get values and with the RF attenuator
\item Efficient and lightweight to achieve clock cycles times of a most \SI{0.1}{\second}
\item Easy implementation of a (time discrete) PID controller
\item GUI to show input, output and error signals
\item Possibility to log signals to file for documentation
\end{itemize}

With these in mind first programming languages are regarded. As there are EPICS libraries for both C++ and Python, these two languages are examined in more detail.\\
While C++ as a compile language promises speed, all other requirements are possible but would take much greater effort in C++ compared to Python. For that reason, in the following a small test program is written to evaluate the fastest clock cycle possible with a simple Python program.

 shows that retrieving one value of an EPICS channel and setting a new attenuator voltage takes only about \SI{20}{\milli\second}, thus using C++ is not necessary and Python can be utilized instead.
 
To create a PID controller in software instead of a continuous time system, only discrete time implementations are possible. Choosing a high clock cycle frequency however approximates the continuous time system. To get the error signals for the integral and derivative part, the integral is replaced with a cumulative recursive sum as
\begin{equation}
e_i[n]=e_i[n-1]+e[n] \cdot dt,
\end{equation}
while the derivative is replaced by a difference
\begin{equation}
e_d[n]=\frac{e_i[n-1]-e[n]}{dt}.
\end{equation}

For the GUI a common framework should be used. In Python Tk and wxwidgets are common ways to build a GUI. Another viable option is PyQt, which, as the name suggests, is a Python port of the Qt framework. One major advantage of using PyQt is the possibility to import \texttt{.ui} files describing the GUI directly from the Qt RAD designer called ``Qt designer'', removing the need to create the GUI programmatically. Furthermore using PyQt enables the usage of \textit{pyqtgraph}, a highspeed plotting library only compatible with Qt. This ensures plotting live data does not bottleneck performance, which is often the problem with naive \textit{matplotlib} based solutions.

To log all relevant data from RAM to non-volatile memory (hard drive or network share), a simple approach with a line by line CSV writer is used.


\section{Control Parameter Tuning and Tests}
To tune control parameters there are a multiple of analytical, empirical or hybrid approaches. Here the Ziegler-Nichols method is tested and fine tuning is done by hand.

\section{Improving the Control System}

\subsection{Adding Disturbance Feed Forward}


\newpage
\section{Results}
To evaluate the performance of the control system, FLUTE is operated with and without the control system switched on for \SI{6}{\hour} each.
Before the test, FLUTE is allowed to run a few hours for all components to reach operating temperatures.
The result of the test is shown in \autoref{fig:controllerDesignAndEvaluation_resultTime}.
Note, that in this test about \SI{7}{\hour} into the experiment there was an unintentional shutdown of FLUTE and the corresponding block of data is removed before further processing.

\begin{figure}[tbh]
	\centering
	\includegraphics[width=\textwidth,height=0.5\textwidth]{chap/ControllerDesignAndEvaluation/img/results/result_timesignal.tikz}
	\caption{Cavity power over about \SI{15}{\hour} (about three hours of downtime removed for clarity around the \SI{7}{\hour} mark); control system switched off at \SI{6}{\hour} (recording started 2021/05/01 20:00)}
	\label{fig:controllerDesignAndEvaluation_resultTime}
\end{figure}

\begin{figure}[tbh]
	\centering
	\includegraphics[width=\textwidth,height=0.5\textwidth]{chap/ControllerDesignAndEvaluation/img/results/result_spectrogram.tikz}
	\caption{Spectrogram of the cavity power in \autoref{fig:controllerDesignAndEvaluation_resultTime}}
	\label{fig:controllerDesignAndEvaluation_resultSpectg}
\end{figure}

The time plot and also the spectogram in \autoref{fig:controllerDesignAndEvaluation_resultSpectg} shows the cavity RF power approximately reaches stationarity in $[0,\,2] \si{hour}$ respectivaley $[6,\,12] \si{hour}$. This allows the spectrum for these two blocks to be estimated using a periodogram method. In \autoref{fig:controllerDesignAndEvaluation_resultSpectg} the spectra for each case are shown.

\begin{figure}[tbh]
	\centering
	\includegraphics[width=\textwidth,height=0.7\textwidth]{chap/ControllerDesignAndEvaluation/img/results/controllerOnVsOff.tikz}
	\caption{Comparison between the control system on and off}
	\label{fig:controllerDesignAndEvaluation_resultComp}
\end{figure}

\begin{figure}[tbh]
    \centering
        \subfloat[Controller off]{\includegraphics[width=\textwidth,height=0.4\textwidth]{chap/ControllerDesignAndEvaluation/img/stdp_off.tikz}}
        \\
        \subfloat[Controller on]{\includegraphics[width=\textwidth,height=0.4\textwidth]{chap/ControllerDesignAndEvaluation/img/stdp_on.tikz}}
       \caption{Relative standard deviation $STD\%(t,T)$}
    \label{fig:controllerDesignAndEvaluation_resultSTDp}
\end{figure}

\begin{figure}[tbh]
	\centering
	\includegraphics[width=\textwidth,height=0.5\textwidth]{chap/ControllerDesignAndEvaluation/img/stdp_means.tikz}
	\caption{Relative standard deviation $STD\%(T)$, shaded areas show $\operatorname{min}\{STD\%(t,T_0)\}$ and $\operatorname{max}\{STD\%(t,T_0)\}$, solid lines show $\operatorname{mean}\{STD\%(t,T_0)\}$ for a fixed window size $T=T_0$}
	\label{fig:controllerDesignAndEvaluation_resultSTDp_means}
\end{figure}


\begin{figure}[tbh]
	\centering
	\includegraphics[width=\textwidth,height=0.7\textwidth]{chap/ControllerDesignAndEvaluation/img/results/controllerOnVsOffSpectrumErrorBand.tikz}
	\caption{Power spectrum of the plots in \autoref{fig:controllerDesignAndEvaluation_resultTime} computed with Welch's method; shaded areas show the uncertainty according to \autoref{eq:varWelch}}
	\label{fig:controllerDesignAndEvaluation_resultPeriodo}
\end{figure}

With the metrics from \autoref{sec:metrics}, the success of the control system is assessed in \autoref{tab:controllerDesignAndEvaluation_metrics}.

This shows for example the mean squared error is improved by a factor of about \num{371} by using the control system.

\begin{table}[tbh]
\centering
\caption{Quantitative assessment of the controllers performance}\label{tab:controllerDesignAndEvaluation_metrics}
\begin{tabular}{lSSS}
	\toprule
	Metric  & {Controller off} & {Controller on} & {Controller off/Controller on} \\ \midrule
	$\%STD$ & 0.0011559        & 5.99225e-05     & 9.2891                         \\
	$MSE$   & 37.639           & 0.10133         & 371.44                         \\
	$MPN$   & 487309.29        & 14386.25        & 33.873                         \\ \bottomrule
\end{tabular}
\end{table}














