\chapter{Theoretical Framework}
\section{Linear Accelerators}
In a linear particle accelerator (LINAC), charged particles such as electrons are accelerated to increase their total energy over their energy at rest.

Compared to heavier particles, such as protons ($m_p=\SI{938.27}{\mega\eV\per c\squared}$), electrons are fairly light ($m_p=\SI{0.511}{\mega\eV\per c\squared}$). Therefore they need to be brought to speeds comparable to the speed of light to achieve an useful energy increase. For this reason relativistic mechanics are needed to describe their movements.\footnote{As relativistic mechanics are a super set of classical mechanics, the equations also apply for slower particles.} \cite{Hinterberger1997}

With the speed of light $c=\SI{2.998e8}{\m\per\s}$ and the particle velocity $v$, it is common to define \cite{Wangler2008}:
\begin{align}\label{eq:theoreticalFramework_gammabeta}
\text{(normalized velocity)}\qquad\beta &= \frac{v}{c}\\
\text{(relativistic mass factor)}\qquad\gamma &= \frac{1}{\sqrt{1-\beta^2}}
\end{align}

The total energy of a particle is
\begin{equation}
U=\underbrace{\left(\gamma-1\right)mc^2}_{\text{kinetic engery } W} + \underbrace{mc^2}_{\text{rest energy}} = \gamma mc^2.
\end{equation}

With the kinetic electron energy out of the FLUTE electron gun of $W=\SI{7}{\mega\eV}$, rearranging \autoref{eq:theoreticalFramework_gammabeta} and using in $W=\left(\gamma-1\right)mc^2$ yield
\begin{align}
\gamma &= \frac{W}{m_e\,c^2}+1=\num{14.699}\\
\beta &= \sqrt{1-\nicefrac{1}{\gamma^2}}=\num{0.99768}.
\end{align}


\section{Beam Diagnostic Devices for Linear Accelerators}
All (linear) particle accelerators need diagnostic devices to monitor the beam sizes, shapes, positions, length or charge.

\paragraph{Beam Position Monitor (BPM)}
\paragraph{Faraday Cup}

\section{Signal Analysis}
\subsection{Auto correlation and Cross correlation}
The \textit{cross covariance} between two stochastic processes $x[n]$ and $y[n]$ is a measure of the similarity between $x[n]$ at index $n_1$ and $y[n]$ at index $n_2$ and is defined as
\begin{equation}\label{eq:crosscovariance}
r_{xy}[n_1,n_2] = \text{E}\left\{(x[n_1]-\mu_x[n_1])(y[n_2]-\mu_y[n_2])^\ast\right\}.
\end{equation}

For the special case of $y[n]:=x[n]$, $r_{xx}[n_1,n_2]$ is called \textit{auto covariance} and is a measure of self similarity of $x[n]$ \cite[p.~172]{Park2017}.

The processes $x[n]$ and $y[n]$ are called \textit{wide sense stationary} (WSS) if the following two properties hold \cite[p.~167]{Park2017}. 
First, their means $\mu_{\xi}[n]$ are constant, i.e. they do not depend on the sample index:
\begin{align}
\mu_{x}[n] &= \mu_x\\
\mu_{y}[n] &= \mu_y
\end{align}
Also the auto covariance does not depend on the absolute sample indices $n_1$ and $n_2$, but merely on the difference between them:
\begin{equation}
r_{xy}[n_1,n_2] = r_{xy}[m],\qquad \text{with: } m:=n_2-n_1
\end{equation}

If both process in \autoref{eq:crosscovariance} are WSS, \autoref{eq:crosscovariance} simplifies to
\begin{equation}
r_{xy}[m] = \text{E}\left\{(x[n]-\mu_x)(y[n-m]-\mu_y)^\ast\right\}.
\end{equation}

For the auto covariance both means are identical and can be moved outside the expectation operator:
\begin{equation}
r_{xx}[m] = \text{E}\left\{(x[n])(y[n-m])^\ast\right\}-\mu_{x}^2.
\end{equation}

When analyzing signals, the stochastic processes are often unknown and only one realization $x[n]$ is known. But if the process generating $x[n]$ is \textit{(weakly) ergodic}, then one realization is enough to determine the auto covariance of the process \cite[p.~252]{Puente2019}.
Then the auto covariance can be estimated with
\begin{equation}\label{eq:autocovarianveEstimation}
\hat{r}[m] = \frac{1}{N} \sum_{n=m+1}^{N} x[n]\,x^\ast[n-m]\qquad m \in [0,\,N-1]
\end{equation} 


\subsection{Estimating the Spectrum of a Stochastic Process}
For a deterministic, time-discrete signal $x[n] \in \mathcal{L}_1$, the discrete Fourier transform (DFT) exists\cite{Lapidoth2019} and is defined as
\begin{equation}
X[k] = \sum_{n=0}^{N-1} x[n]\,\text{e}^{-j\frac{2\pi}{N}k\,n}\qquad k,n \in [0,\,N-1],
\end{equation}
using $k=\frac{N}{2\pi}\,\omega = N\,f$ as the independent, discrete frequency variable. 
From the complex sequence $X[k]$, often only the magnitude (or energy) is of greater interest while the phase information are neglected. 
Therefore, $S_{xx}$ is defined as
\begin{equation}
S_{xx} = \left|X[k]\right|^2
\end{equation}
and called the \textit{energy spectral density (ESD)}.

If $x[n]$ is the realization of a stochastic process, then it is of random nature rather than deterministic.
Because realizations of physical processes do not posses finite energy, they are not in the $\mathcal{L}_1$ set and their DFT is not defined \cite[p.~5]{Stoica1997}.

In this case instead of an energy spectral density, the spectrum of the average power of the process, called the \textit{power spectral density (PSD)}, can be used instead.
To compute the PSD, either there are two possibilities:
\begin{align}
\Phi_{xx}[k] &= \sum_{m=-\infty}^{\infty} r[m]\,\text{e}^{-j\frac{2\pi}{N}k\,m} \label{eq:phixx1}\\
\Phi_{xx}[k] &= \lim_{N\rightarrow\infty} \text{E}\left\{\frac{1}{N}\left|\sum_{n=0}^{N-1} x[n]\,\text{e}^{-j\frac{2\pi}{N}k\,n}\right|^2 \right\} \label{eq:phixx2}
\end{align}
When assuming $r[m]$ decays ``fast enough'', i.e.
\begin{equation}
\lim_{N\rightarrow\infty} \frac{1}{N} \sum_{m=-N}^{N} |m|\,\left|r[m]\right| = 0
\end{equation}
then \autoref{eq:phixx1} and \autoref{eq:phixx2} are equal\cite[p.~7]{Stoica1997}.

For measured data however neither equations can be used directly.
For \autoref{eq:phixx1} the auto covariance sequence $r[m]$ is unknown.
But it could be estimated with \autoref{eq:autocovarianveEstimation}. In case of \autoref{eq:phixx2} it is not possible to evaluate the limit, because only finite length data can be sampled and also the expectation can not be computed since in general there is only one realization available. Both operations can be neglected when doing an estimation.

With these practical changes in place, \autoref{eq:phixx1} and \autoref{eq:phixx2} become
\begin{align}
\hat{\Phi}_{c,\,xx}[k] &= \sum_{m=-(N-1)}^{N-1} \hat{r}[m]\,\text{e}^{-j\frac{2\pi}{N}k\,m} \label{eq:phiCxx1}\qquad\text{(Correlogram)}\\
\hat{\Phi}_{p,\,xx}[k] &= \frac{1}{N} \left| \sum_{n=0}^{N-1} x[n]\,\text{e}^{-j\frac{2\pi}{N}k\,n}\right|^2\qquad\text{(Periodogram)}\label{eq:periodogram}.
\end{align}

Both methods yield equal results, if $r[m]$ is estimated with the biased estimator $\hat{r}[m]$ in \autoref{eq:autocovarianveEstimation} in contrast to the unbiased estimator (compare \cite[p.~24]{Stoica1997})
\begin{equation}
\hat{r}_{\text{unbiased}}[m] = \frac{1}{N-m} \sum_{n=m+1}^{N} x[n]\,x^\ast[n-m]\qquad m \in [0,\,N-1].
\end{equation}

\cite{Rowell2008} shows one key weakness of the unmodified periodogram method in \autoref{eq:periodogram}: The variance does not decrease significantly with more samples $N$. Instead the variance of the periodogram for each frequency approaches the square of the actual PSD:
\begin{equation}
\lim_{N\rightarrow\infty} \text{Var}\left\{\hat{\Phi}_{p,\,xx}[k]\right\} = \Phi_{xx}^2[k]
\end{equation}
Furthermore the periodogram/correlogram suffer from the smearing and leakage effects because the limited length of the data samples always causes an implicit windowing, thus reducing frequency resolution.\\

There are several popular methods that improve on the periodogram/correlogram concepts:

\textbf{Blackman-Tukey:} Because of the poor accuracy of $\hat{r}[m]$ for $k\approx N$ in the definition of $\hat{\Phi}_{c,\,xx}[k]$ and the bigger the $N$, the more small errors in $\hat{r}[m]$ sum up, truncating/windowing of $\hat{r}[m]$ with $w[k]$ (length $M$) can be beneficial for the accuracy of the estimation.
\begin{equation}
\hat{\Phi}_{BT,\,xx}[k] = \sum_{m=-(M-1)}^{M-1} w[k]\hat{r}[m]\,\text{e}^{-j\frac{2\pi}{N}k\,m}
\end{equation}
The choice of the window $w[k]$ trades frequency resolution for variance and smearing for leakage reduction \cite[p.~41]{Stoica1997}.

\textbf{Barlett:} The Barlett method reduces the variance of the periodogram by splitting the $N$ data samples in $Q=\nicefrac{N}{M}$ blocks and averaging together the sub-periodograms:
\begin{align}
\hat{\Phi}_{q,\,xx}[k] &= \frac{1}{M} \left| \sum_{n=0}^{M-1} x_q[n]\,\text{e}^{-j\frac{2\pi}{M}k\,n}\right|^2\\
\hat{\Phi}_{B,\,xx}[k] &= \frac{1}{Q} \sum_{q=1}^{Q} \hat{\Phi}_{q,\,xx}[k]
\end{align}
The variance of the estimation scales with $Q$ \cite[p.~6]{Rowell2008}:
\begin{equation}
\text{Var}\left\{\hat{\Phi}_{B,\,xx}[k]\right\} = \frac{1}{Q}\Phi_{xx}^2[k]
\end{equation}

\textbf{Welch:} The Welch method combines splitting the data into $Q$ segments with windowing each segment and allowing the segments to overlap. With $P = \nicefrac{1}{M} \sum_{n=0}^{M-1} |w[n]|^2$ being the ``power'' of the window, the Welch method is computed as
\begin{align}
\hat{\Phi}_{s,\,xx}[k] &= \frac{1}{M P} \left| \sum_{n=0}^{M-1} x_s[n]\,\text{e}^{-j\frac{2\pi}{M}k\,n}\right|^2\\
\hat{\Phi}_{W,\,xx}[k] &= \frac{1}{Q} \sum_{s=1}^{Q} \hat{\Phi}_{s,\,xx}[k].
\end{align}
Compared to the Barlett method, the overlapping of up to \SI{50}{\percent} (see \cite{Welch1967}) allows increasing $Q$, thus reducing the variance.
\begin{equation}\label{eq:varWelch}
\text{Var}\left\{\hat{\Phi}_{W,\,xx}[k]\right\} = \frac{1}{Q}\Phi_{xx}^2[k]
\end{equation}


In case on a non-stationary signal $x[n]$, one possibility to analyze and display the spectral content is the use of the short time Fourier transform (STFT) and the spectrogram, which is a two dimensional power spectral density function mapping frequency and time to a third coordinate like height, intensity or color.

To calculate the spectrogram, the signal is split into segments with the sliding window $w[n-m]$ for which duration the signal is assumed to be stationary. For each segment at time index $m$, the periodogram is calculated according to
\begin{equation}
\hat{\Phi}_{xx}[k,m] = \frac{1}{N} \left| \sum_{n=0}^{N-1} w[n-m] x[n]\,\text{e}^{-j\frac{2\pi}{N}k\,n}\right|^2.
\end{equation}

\section{Feedback Control Systems}







